{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load point data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename    = 'assignment_06_data.csv'\n",
    "data_load   = np.loadtxt(filename, delimiter = ',')\n",
    "\n",
    "x   = data_load[0, :]\n",
    "y   = data_load[1, :]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.plot(x, y, '.', color = 'blue')\n",
    "plt.title('data points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y, theta0, theta1):\n",
    "    A = np.vander(x,2, increasing = True)\n",
    "    theta = np.array([theta0, theta1])\n",
    "    loss = y - np.matmul(A,theta)\n",
    "    loss = np.sum(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the gradient for each model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_theta0(x, y, theta0, theta1):\n",
    "    loss = compute_loss(x,y,theta0,theta1)\n",
    "    dL = (2/len(x))*(-loss)\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_theta1(x, y, theta0, theta1):\n",
    "    A = np.vander(x,2, increasing = True)\n",
    "    theta = np.array([theta0, theta1])\n",
    "    loss_1 = y - np.matmul(A,theta)\n",
    "    loss_2 = loss_1 * x\n",
    "    loss = np.sum(loss_2)\n",
    "    dL = (2/len(x))*(-loss)\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent for each model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =    0, loss = 810.77735\n",
      "iteration =    1, loss = 794.56180\n",
      "iteration =    2, loss = 778.67057\n",
      "iteration =    3, loss = 763.09716\n",
      "iteration =    4, loss = 747.83521\n",
      "iteration =    5, loss = 732.87851\n",
      "iteration =    6, loss = 718.22094\n",
      "iteration =    7, loss = 703.85652\n",
      "iteration =    8, loss = 689.77939\n",
      "iteration =    9, loss = 675.98380\n",
      "iteration =   10, loss = 662.46412\n",
      "iteration =   11, loss = 649.21484\n",
      "iteration =   12, loss = 636.23055\n",
      "iteration =   13, loss = 623.50593\n",
      "iteration =   14, loss = 611.03582\n",
      "iteration =   15, loss = 598.81510\n",
      "iteration =   16, loss = 586.83880\n",
      "iteration =   17, loss = 575.10202\n",
      "iteration =   18, loss = 563.59998\n",
      "iteration =   19, loss = 552.32798\n",
      "iteration =   20, loss = 541.28142\n",
      "iteration =   21, loss = 530.45579\n",
      "iteration =   22, loss = 519.84668\n",
      "iteration =   23, loss = 509.44974\n",
      "iteration =   24, loss = 499.26075\n",
      "iteration =   25, loss = 489.27553\n",
      "iteration =   26, loss = 479.49002\n",
      "iteration =   27, loss = 469.90022\n",
      "iteration =   28, loss = 460.50222\n",
      "iteration =   29, loss = 451.29217\n",
      "iteration =   30, loss = 442.26633\n",
      "iteration =   31, loss = 433.42100\n",
      "iteration =   32, loss = 424.75258\n",
      "iteration =   33, loss = 416.25753\n",
      "iteration =   34, loss = 407.93238\n",
      "iteration =   35, loss = 399.77373\n",
      "iteration =   36, loss = 391.77826\n",
      "iteration =   37, loss = 383.94269\n",
      "iteration =   38, loss = 376.26384\n",
      "iteration =   39, loss = 368.73856\n",
      "iteration =   40, loss = 361.36379\n",
      "iteration =   41, loss = 354.13652\n",
      "iteration =   42, loss = 347.05379\n",
      "iteration =   43, loss = 340.11271\n",
      "iteration =   44, loss = 333.31046\n",
      "iteration =   45, loss = 326.64425\n",
      "iteration =   46, loss = 320.11136\n",
      "iteration =   47, loss = 313.70913\n",
      "iteration =   48, loss = 307.43495\n",
      "iteration =   49, loss = 301.28625\n",
      "iteration =   50, loss = 295.26053\n",
      "iteration =   51, loss = 289.35532\n",
      "iteration =   52, loss = 283.56821\n",
      "iteration =   53, loss = 277.89685\n",
      "iteration =   54, loss = 272.33891\n",
      "iteration =   55, loss = 266.89213\n",
      "iteration =   56, loss = 261.55429\n",
      "iteration =   57, loss = 256.32320\n",
      "iteration =   58, loss = 251.19674\n",
      "iteration =   59, loss = 246.17280\n",
      "iteration =   60, loss = 241.24935\n",
      "iteration =   61, loss = 236.42436\n",
      "iteration =   62, loss = 231.69587\n",
      "iteration =   63, loss = 227.06196\n",
      "iteration =   64, loss = 222.52072\n",
      "iteration =   65, loss = 218.07030\n",
      "iteration =   66, loss = 213.70890\n",
      "iteration =   67, loss = 209.43472\n",
      "iteration =   68, loss = 205.24602\n",
      "iteration =   69, loss = 201.14110\n",
      "iteration =   70, loss = 197.11828\n",
      "iteration =   71, loss = 193.17592\n",
      "iteration =   72, loss = 189.31240\n",
      "iteration =   73, loss = 185.52615\n",
      "iteration =   74, loss = 181.81563\n",
      "iteration =   75, loss = 178.17931\n",
      "iteration =   76, loss = 174.61573\n",
      "iteration =   77, loss = 171.12341\n",
      "iteration =   78, loss = 167.70095\n",
      "iteration =   79, loss = 164.34693\n",
      "iteration =   80, loss = 161.05999\n",
      "iteration =   81, loss = 157.83879\n",
      "iteration =   82, loss = 154.68201\n",
      "iteration =   83, loss = 151.58837\n",
      "iteration =   84, loss = 148.55660\n",
      "iteration =   85, loss = 145.58547\n",
      "iteration =   86, loss = 142.67376\n",
      "iteration =   87, loss = 139.82029\n",
      "iteration =   88, loss = 137.02388\n",
      "iteration =   89, loss = 134.28340\n",
      "iteration =   90, loss = 131.59774\n",
      "iteration =   91, loss = 128.96578\n",
      "iteration =   92, loss = 126.38647\n",
      "iteration =   93, loss = 123.85874\n",
      "iteration =   94, loss = 121.38156\n",
      "iteration =   95, loss = 118.95393\n",
      "iteration =   96, loss = 116.57485\n",
      "iteration =   97, loss = 114.24336\n",
      "iteration =   98, loss = 111.95849\n",
      "iteration =   99, loss = 109.71932\n",
      "iteration =  100, loss = 107.52493\n",
      "iteration =  101, loss = 105.37443\n",
      "iteration =  102, loss = 103.26694\n",
      "iteration =  103, loss = 101.20161\n",
      "iteration =  104, loss = 99.17757\n",
      "iteration =  105, loss = 97.19402\n",
      "iteration =  106, loss = 95.25014\n",
      "iteration =  107, loss = 93.34514\n",
      "iteration =  108, loss = 91.47824\n",
      "iteration =  109, loss = 89.64867\n",
      "iteration =  110, loss = 87.85570\n",
      "iteration =  111, loss = 86.09858\n",
      "iteration =  112, loss = 84.37661\n",
      "iteration =  113, loss = 82.68908\n",
      "iteration =  114, loss = 81.03530\n",
      "iteration =  115, loss = 79.41459\n",
      "iteration =  116, loss = 77.82630\n",
      "iteration =  117, loss = 76.26977\n",
      "iteration =  118, loss = 74.74438\n",
      "iteration =  119, loss = 73.24949\n",
      "iteration =  120, loss = 71.78450\n",
      "iteration =  121, loss = 70.34881\n",
      "iteration =  122, loss = 68.94184\n",
      "iteration =  123, loss = 67.56300\n",
      "iteration =  124, loss = 66.21174\n",
      "iteration =  125, loss = 64.88750\n",
      "iteration =  126, loss = 63.58975\n",
      "iteration =  127, loss = 62.31796\n",
      "iteration =  128, loss = 61.07160\n",
      "iteration =  129, loss = 59.85017\n",
      "iteration =  130, loss = 58.65316\n",
      "iteration =  131, loss = 57.48010\n",
      "iteration =  132, loss = 56.33050\n",
      "iteration =  133, loss = 55.20389\n",
      "iteration =  134, loss = 54.09981\n",
      "iteration =  135, loss = 53.01782\n",
      "iteration =  136, loss = 51.95746\n",
      "iteration =  137, loss = 50.91831\n",
      "iteration =  138, loss = 49.89994\n",
      "iteration =  139, loss = 48.90194\n",
      "iteration =  140, loss = 47.92391\n",
      "iteration =  141, loss = 46.96543\n",
      "iteration =  142, loss = 46.02612\n",
      "iteration =  143, loss = 45.10560\n",
      "iteration =  144, loss = 44.20348\n",
      "iteration =  145, loss = 43.31941\n",
      "iteration =  146, loss = 42.45303\n",
      "iteration =  147, loss = 41.60397\n",
      "iteration =  148, loss = 40.77189\n",
      "iteration =  149, loss = 39.95645\n",
      "iteration =  150, loss = 39.15732\n",
      "iteration =  151, loss = 38.37417\n",
      "iteration =  152, loss = 37.60669\n",
      "iteration =  153, loss = 36.85456\n",
      "iteration =  154, loss = 36.11747\n",
      "iteration =  155, loss = 35.39512\n",
      "iteration =  156, loss = 34.68721\n",
      "iteration =  157, loss = 33.99347\n",
      "iteration =  158, loss = 33.31360\n",
      "iteration =  159, loss = 32.64733\n",
      "iteration =  160, loss = 31.99438\n",
      "iteration =  161, loss = 31.35449\n",
      "iteration =  162, loss = 30.72740\n",
      "iteration =  163, loss = 30.11286\n",
      "iteration =  164, loss = 29.51060\n",
      "iteration =  165, loss = 28.92039\n",
      "iteration =  166, loss = 28.34198\n",
      "iteration =  167, loss = 27.77514\n",
      "iteration =  168, loss = 27.21964\n",
      "iteration =  169, loss = 26.67524\n",
      "iteration =  170, loss = 26.14174\n",
      "iteration =  171, loss = 25.61890\n",
      "iteration =  172, loss = 25.10653\n",
      "iteration =  173, loss = 24.60440\n",
      "iteration =  174, loss = 24.11231\n",
      "iteration =  175, loss = 23.63006\n",
      "iteration =  176, loss = 23.15746\n",
      "iteration =  177, loss = 22.69431\n",
      "iteration =  178, loss = 22.24042\n",
      "iteration =  179, loss = 21.79562\n",
      "iteration =  180, loss = 21.35970\n",
      "iteration =  181, loss = 20.93251\n",
      "iteration =  182, loss = 20.51386\n",
      "iteration =  183, loss = 20.10358\n",
      "iteration =  184, loss = 19.70151\n",
      "iteration =  185, loss = 19.30748\n",
      "iteration =  186, loss = 18.92133\n",
      "iteration =  187, loss = 18.54290\n",
      "iteration =  188, loss = 18.17205\n",
      "iteration =  189, loss = 17.80861\n",
      "iteration =  190, loss = 17.45243\n",
      "iteration =  191, loss = 17.10338\n",
      "iteration =  192, loss = 16.76132\n",
      "iteration =  193, loss = 16.42609\n",
      "iteration =  194, loss = 16.09757\n",
      "iteration =  195, loss = 15.77562\n",
      "iteration =  196, loss = 15.46011\n",
      "iteration =  197, loss = 15.15090\n",
      "iteration =  198, loss = 14.84788\n",
      "iteration =  199, loss = 14.55093\n",
      "iteration =  200, loss = 14.25991\n",
      "iteration =  201, loss = 13.97471\n",
      "iteration =  202, loss = 13.69522\n",
      "iteration =  203, loss = 13.42131\n",
      "iteration =  204, loss = 13.15289\n",
      "iteration =  205, loss = 12.88983\n",
      "iteration =  206, loss = 12.63203\n",
      "iteration =  207, loss = 12.37939\n",
      "iteration =  208, loss = 12.13180\n",
      "iteration =  209, loss = 11.88917\n",
      "iteration =  210, loss = 11.65138\n",
      "iteration =  211, loss = 11.41836\n",
      "iteration =  212, loss = 11.18999\n",
      "iteration =  213, loss = 10.96619\n",
      "iteration =  214, loss = 10.74687\n",
      "iteration =  215, loss = 10.53193\n",
      "iteration =  216, loss = 10.32129\n",
      "iteration =  217, loss = 10.11486\n",
      "iteration =  218, loss = 9.91257\n",
      "iteration =  219, loss = 9.71432\n",
      "iteration =  220, loss = 9.52003\n",
      "iteration =  221, loss = 9.32963\n",
      "iteration =  222, loss = 9.14304\n",
      "iteration =  223, loss = 8.96017\n",
      "iteration =  224, loss = 8.78097\n",
      "iteration =  225, loss = 8.60535\n",
      "iteration =  226, loss = 8.43324\n",
      "iteration =  227, loss = 8.26458\n",
      "iteration =  228, loss = 8.09929\n",
      "iteration =  229, loss = 7.93730\n",
      "iteration =  230, loss = 7.77856\n",
      "iteration =  231, loss = 7.62299\n",
      "iteration =  232, loss = 7.47053\n",
      "iteration =  233, loss = 7.32112\n",
      "iteration =  234, loss = 7.17469\n",
      "iteration =  235, loss = 7.03120\n",
      "iteration =  236, loss = 6.89058\n",
      "iteration =  237, loss = 6.75276\n",
      "iteration =  238, loss = 6.61771\n",
      "iteration =  239, loss = 6.48535\n",
      "iteration =  240, loss = 6.35565\n",
      "iteration =  241, loss = 6.22853\n",
      "iteration =  242, loss = 6.10396\n",
      "iteration =  243, loss = 5.98188\n",
      "iteration =  244, loss = 5.86225\n",
      "iteration =  245, loss = 5.74500\n",
      "iteration =  246, loss = 5.63010\n",
      "iteration =  247, loss = 5.51750\n",
      "iteration =  248, loss = 5.40715\n",
      "iteration =  249, loss = 5.29901\n",
      "iteration =  250, loss = 5.19303\n",
      "iteration =  251, loss = 5.08917\n",
      "iteration =  252, loss = 4.98738\n",
      "iteration =  253, loss = 4.88763\n",
      "iteration =  254, loss = 4.78988\n",
      "iteration =  255, loss = 4.69408\n",
      "iteration =  256, loss = 4.60020\n",
      "iteration =  257, loss = 4.50820\n",
      "iteration =  258, loss = 4.41803\n",
      "iteration =  259, loss = 4.32967\n",
      "iteration =  260, loss = 4.24308\n",
      "iteration =  261, loss = 4.15822\n",
      "iteration =  262, loss = 4.07505\n",
      "iteration =  263, loss = 3.99355\n",
      "iteration =  264, loss = 3.91368\n",
      "iteration =  265, loss = 3.83541\n",
      "iteration =  266, loss = 3.75870\n",
      "iteration =  267, loss = 3.68353\n",
      "iteration =  268, loss = 3.60986\n",
      "iteration =  269, loss = 3.53766\n",
      "iteration =  270, loss = 3.46691\n",
      "iteration =  271, loss = 3.39757\n",
      "iteration =  272, loss = 3.32962\n",
      "iteration =  273, loss = 3.26302\n",
      "iteration =  274, loss = 3.19776\n",
      "iteration =  275, loss = 3.13381\n",
      "iteration =  276, loss = 3.07113\n",
      "iteration =  277, loss = 3.00971\n",
      "iteration =  278, loss = 2.94952\n",
      "iteration =  279, loss = 2.89052\n",
      "iteration =  280, loss = 2.83271\n",
      "iteration =  281, loss = 2.77606\n",
      "iteration =  282, loss = 2.72054\n",
      "iteration =  283, loss = 2.66613\n",
      "iteration =  284, loss = 2.61281\n",
      "iteration =  285, loss = 2.56055\n",
      "iteration =  286, loss = 2.50934\n",
      "iteration =  287, loss = 2.45915\n",
      "iteration =  288, loss = 2.40997\n",
      "iteration =  289, loss = 2.36177\n",
      "iteration =  290, loss = 2.31453\n",
      "iteration =  291, loss = 2.26824\n",
      "iteration =  292, loss = 2.22288\n",
      "iteration =  293, loss = 2.17842\n",
      "iteration =  294, loss = 2.13485\n",
      "iteration =  295, loss = 2.09216\n",
      "iteration =  296, loss = 2.05031\n",
      "iteration =  297, loss = 2.00931\n",
      "iteration =  298, loss = 1.96912\n",
      "iteration =  299, loss = 1.92974\n",
      "iteration =  300, loss = 1.89114\n",
      "iteration =  301, loss = 1.85332\n",
      "iteration =  302, loss = 1.81625\n",
      "iteration =  303, loss = 1.77993\n",
      "iteration =  304, loss = 1.74433\n",
      "iteration =  305, loss = 1.70944\n",
      "iteration =  306, loss = 1.67525\n",
      "iteration =  307, loss = 1.64175\n",
      "iteration =  308, loss = 1.60891\n",
      "iteration =  309, loss = 1.57674\n",
      "iteration =  310, loss = 1.54520\n",
      "iteration =  311, loss = 1.51430\n",
      "iteration =  312, loss = 1.48401\n",
      "iteration =  313, loss = 1.45433\n",
      "iteration =  314, loss = 1.42524\n",
      "iteration =  315, loss = 1.39674\n",
      "iteration =  316, loss = 1.36880\n",
      "iteration =  317, loss = 1.34143\n",
      "iteration =  318, loss = 1.31460\n",
      "iteration =  319, loss = 1.28831\n",
      "iteration =  320, loss = 1.26254\n",
      "iteration =  321, loss = 1.23729\n",
      "iteration =  322, loss = 1.21255\n",
      "iteration =  323, loss = 1.18829\n",
      "iteration =  324, loss = 1.16453\n",
      "iteration =  325, loss = 1.14124\n",
      "iteration =  326, loss = 1.11841\n",
      "iteration =  327, loss = 1.09604\n",
      "iteration =  328, loss = 1.07412\n",
      "iteration =  329, loss = 1.05264\n",
      "iteration =  330, loss = 1.03159\n",
      "iteration =  331, loss = 1.01096\n",
      "iteration =  332, loss = 0.99074\n",
      "iteration =  333, loss = 0.97092\n",
      "iteration =  334, loss = 0.95150\n",
      "iteration =  335, loss = 0.93247\n",
      "iteration =  336, loss = 0.91383\n",
      "iteration =  337, loss = 0.89555\n",
      "iteration =  338, loss = 0.87764\n",
      "iteration =  339, loss = 0.86008\n",
      "iteration =  340, loss = 0.84288\n",
      "iteration =  341, loss = 0.82603\n",
      "iteration =  342, loss = 0.80950\n",
      "iteration =  343, loss = 0.79331\n",
      "iteration =  344, loss = 0.77745\n",
      "iteration =  345, loss = 0.76190\n",
      "iteration =  346, loss = 0.74666\n",
      "iteration =  347, loss = 0.73173\n",
      "iteration =  348, loss = 0.71709\n",
      "iteration =  349, loss = 0.70275\n",
      "iteration =  350, loss = 0.68870\n",
      "iteration =  351, loss = 0.67492\n",
      "iteration =  352, loss = 0.66142\n",
      "iteration =  353, loss = 0.64820\n",
      "iteration =  354, loss = 0.63523\n",
      "iteration =  355, loss = 0.62253\n",
      "iteration =  356, loss = 0.61008\n",
      "iteration =  357, loss = 0.59788\n",
      "iteration =  358, loss = 0.58592\n",
      "iteration =  359, loss = 0.57420\n",
      "iteration =  360, loss = 0.56272\n",
      "iteration =  361, loss = 0.55146\n",
      "iteration =  362, loss = 0.54043\n",
      "iteration =  363, loss = 0.52962\n",
      "iteration =  364, loss = 0.51903\n",
      "iteration =  365, loss = 0.50865\n",
      "iteration =  366, loss = 0.49848\n",
      "iteration =  367, loss = 0.48851\n",
      "iteration =  368, loss = 0.47874\n",
      "iteration =  369, loss = 0.46916\n",
      "iteration =  370, loss = 0.45978\n",
      "iteration =  371, loss = 0.45058\n",
      "iteration =  372, loss = 0.44157\n",
      "iteration =  373, loss = 0.43274\n",
      "iteration =  374, loss = 0.42409\n",
      "iteration =  375, loss = 0.41560\n",
      "iteration =  376, loss = 0.40729\n",
      "iteration =  377, loss = 0.39915\n",
      "iteration =  378, loss = 0.39116\n",
      "iteration =  379, loss = 0.38334\n",
      "iteration =  380, loss = 0.37567\n",
      "iteration =  381, loss = 0.36816\n",
      "iteration =  382, loss = 0.36080\n",
      "iteration =  383, loss = 0.35358\n",
      "iteration =  384, loss = 0.34651\n",
      "iteration =  385, loss = 0.33958\n",
      "iteration =  386, loss = 0.33279\n",
      "iteration =  387, loss = 0.32613\n",
      "iteration =  388, loss = 0.31961\n",
      "iteration =  389, loss = 0.31322\n",
      "iteration =  390, loss = 0.30695\n",
      "iteration =  391, loss = 0.30081\n",
      "iteration =  392, loss = 0.29480\n",
      "iteration =  393, loss = 0.28890\n",
      "iteration =  394, loss = 0.28312\n",
      "iteration =  395, loss = 0.27746\n",
      "iteration =  396, loss = 0.27191\n",
      "iteration =  397, loss = 0.26647\n",
      "iteration =  398, loss = 0.26114\n",
      "iteration =  399, loss = 0.25592\n",
      "iteration =  400, loss = 0.25080\n",
      "iteration =  401, loss = 0.24579\n",
      "iteration =  402, loss = 0.24087\n",
      "iteration =  403, loss = 0.23605\n",
      "iteration =  404, loss = 0.23133\n",
      "iteration =  405, loss = 0.22671\n",
      "iteration =  406, loss = 0.22217\n",
      "iteration =  407, loss = 0.21773\n",
      "iteration =  408, loss = 0.21337\n",
      "iteration =  409, loss = 0.20911\n",
      "iteration =  410, loss = 0.20492\n",
      "iteration =  411, loss = 0.20083\n",
      "iteration =  412, loss = 0.19681\n",
      "iteration =  413, loss = 0.19287\n",
      "iteration =  414, loss = 0.18902\n",
      "iteration =  415, loss = 0.18523\n",
      "iteration =  416, loss = 0.18153\n",
      "iteration =  417, loss = 0.17790\n",
      "iteration =  418, loss = 0.17434\n",
      "iteration =  419, loss = 0.17085\n",
      "iteration =  420, loss = 0.16744\n",
      "iteration =  421, loss = 0.16409\n",
      "iteration =  422, loss = 0.16081\n",
      "iteration =  423, loss = 0.15759\n",
      "iteration =  424, loss = 0.15444\n",
      "iteration =  425, loss = 0.15135\n",
      "iteration =  426, loss = 0.14832\n",
      "iteration =  427, loss = 0.14536\n",
      "iteration =  428, loss = 0.14245\n",
      "iteration =  429, loss = 0.13960\n",
      "iteration =  430, loss = 0.13681\n",
      "iteration =  431, loss = 0.13407\n",
      "iteration =  432, loss = 0.13139\n",
      "iteration =  433, loss = 0.12876\n",
      "iteration =  434, loss = 0.12619\n",
      "iteration =  435, loss = 0.12366\n",
      "iteration =  436, loss = 0.12119\n",
      "iteration =  437, loss = 0.11877\n",
      "iteration =  438, loss = 0.11639\n",
      "iteration =  439, loss = 0.11406\n",
      "iteration =  440, loss = 0.11178\n",
      "iteration =  441, loss = 0.10955\n",
      "iteration =  442, loss = 0.10736\n",
      "iteration =  443, loss = 0.10521\n",
      "iteration =  444, loss = 0.10310\n",
      "iteration =  445, loss = 0.10104\n",
      "iteration =  446, loss = 0.09902\n",
      "iteration =  447, loss = 0.09704\n",
      "iteration =  448, loss = 0.09510\n",
      "iteration =  449, loss = 0.09320\n",
      "iteration =  450, loss = 0.09133\n",
      "iteration =  451, loss = 0.08951\n",
      "iteration =  452, loss = 0.08772\n",
      "iteration =  453, loss = 0.08596\n",
      "iteration =  454, loss = 0.08424\n",
      "iteration =  455, loss = 0.08256\n",
      "iteration =  456, loss = 0.08091\n",
      "iteration =  457, loss = 0.07929\n",
      "iteration =  458, loss = 0.07770\n",
      "iteration =  459, loss = 0.07615\n",
      "iteration =  460, loss = 0.07463\n",
      "iteration =  461, loss = 0.07313\n",
      "iteration =  462, loss = 0.07167\n",
      "iteration =  463, loss = 0.07024\n",
      "iteration =  464, loss = 0.06883\n",
      "iteration =  465, loss = 0.06746\n",
      "iteration =  466, loss = 0.06611\n",
      "iteration =  467, loss = 0.06479\n",
      "iteration =  468, loss = 0.06349\n",
      "iteration =  469, loss = 0.06222\n",
      "iteration =  470, loss = 0.06098\n",
      "iteration =  471, loss = 0.05976\n",
      "iteration =  472, loss = 0.05856\n",
      "iteration =  473, loss = 0.05739\n",
      "iteration =  474, loss = 0.05624\n",
      "iteration =  475, loss = 0.05512\n",
      "iteration =  476, loss = 0.05401\n",
      "iteration =  477, loss = 0.05293\n",
      "iteration =  478, loss = 0.05188\n",
      "iteration =  479, loss = 0.05084\n",
      "iteration =  480, loss = 0.04982\n",
      "iteration =  481, loss = 0.04883\n",
      "iteration =  482, loss = 0.04785\n",
      "iteration =  483, loss = 0.04689\n",
      "iteration =  484, loss = 0.04595\n",
      "iteration =  485, loss = 0.04503\n",
      "iteration =  486, loss = 0.04413\n",
      "iteration =  487, loss = 0.04325\n",
      "iteration =  488, loss = 0.04239\n",
      "iteration =  489, loss = 0.04154\n",
      "iteration =  490, loss = 0.04071\n",
      "iteration =  491, loss = 0.03989\n",
      "iteration =  492, loss = 0.03910\n",
      "iteration =  493, loss = 0.03831\n",
      "iteration =  494, loss = 0.03755\n",
      "iteration =  495, loss = 0.03680\n",
      "iteration =  496, loss = 0.03606\n",
      "iteration =  497, loss = 0.03534\n",
      "iteration =  498, loss = 0.03463\n",
      "iteration =  499, loss = 0.03394\n",
      "iteration =  500, loss = 0.03326\n",
      "iteration =  501, loss = 0.03260\n",
      "iteration =  502, loss = 0.03194\n",
      "iteration =  503, loss = 0.03131\n",
      "iteration =  504, loss = 0.03068\n",
      "iteration =  505, loss = 0.03007\n",
      "iteration =  506, loss = 0.02946\n",
      "iteration =  507, loss = 0.02887\n",
      "iteration =  508, loss = 0.02830\n",
      "iteration =  509, loss = 0.02773\n",
      "iteration =  510, loss = 0.02718\n",
      "iteration =  511, loss = 0.02663\n",
      "iteration =  512, loss = 0.02610\n",
      "iteration =  513, loss = 0.02558\n",
      "iteration =  514, loss = 0.02507\n",
      "iteration =  515, loss = 0.02457\n",
      "iteration =  516, loss = 0.02407\n",
      "iteration =  517, loss = 0.02359\n",
      "iteration =  518, loss = 0.02312\n",
      "iteration =  519, loss = 0.02266\n",
      "iteration =  520, loss = 0.02221\n",
      "iteration =  521, loss = 0.02176\n",
      "iteration =  522, loss = 0.02133\n",
      "iteration =  523, loss = 0.02090\n",
      "iteration =  524, loss = 0.02048\n",
      "iteration =  525, loss = 0.02007\n",
      "iteration =  526, loss = 0.01967\n",
      "iteration =  527, loss = 0.01928\n",
      "iteration =  528, loss = 0.01889\n",
      "iteration =  529, loss = 0.01851\n",
      "iteration =  530, loss = 0.01814\n",
      "iteration =  531, loss = 0.01778\n",
      "iteration =  532, loss = 0.01743\n",
      "iteration =  533, loss = 0.01708\n",
      "iteration =  534, loss = 0.01674\n",
      "iteration =  535, loss = 0.01640\n",
      "iteration =  536, loss = 0.01607\n",
      "iteration =  537, loss = 0.01575\n",
      "iteration =  538, loss = 0.01544\n",
      "iteration =  539, loss = 0.01513\n",
      "iteration =  540, loss = 0.01482\n",
      "iteration =  541, loss = 0.01453\n",
      "iteration =  542, loss = 0.01424\n",
      "iteration =  543, loss = 0.01395\n",
      "iteration =  544, loss = 0.01367\n",
      "iteration =  545, loss = 0.01340\n",
      "iteration =  546, loss = 0.01313\n",
      "iteration =  547, loss = 0.01287\n",
      "iteration =  548, loss = 0.01261\n",
      "iteration =  549, loss = 0.01236\n",
      "iteration =  550, loss = 0.01211\n",
      "iteration =  551, loss = 0.01187\n",
      "iteration =  552, loss = 0.01163\n",
      "iteration =  553, loss = 0.01140\n",
      "iteration =  554, loss = 0.01117\n",
      "iteration =  555, loss = 0.01095\n",
      "iteration =  556, loss = 0.01073\n",
      "iteration =  557, loss = 0.01052\n",
      "iteration =  558, loss = 0.01031\n",
      "iteration =  559, loss = 0.01010\n",
      "iteration =  560, loss = 0.00990\n",
      "iteration =  561, loss = 0.00970\n",
      "iteration =  562, loss = 0.00951\n",
      "iteration =  563, loss = 0.00931\n",
      "iteration =  564, loss = 0.00913\n",
      "iteration =  565, loss = 0.00895\n",
      "iteration =  566, loss = 0.00877\n",
      "iteration =  567, loss = 0.00859\n",
      "iteration =  568, loss = 0.00842\n",
      "iteration =  569, loss = 0.00825\n",
      "iteration =  570, loss = 0.00809\n",
      "iteration =  571, loss = 0.00792\n",
      "iteration =  572, loss = 0.00777\n",
      "iteration =  573, loss = 0.00761\n",
      "iteration =  574, loss = 0.00746\n",
      "iteration =  575, loss = 0.00731\n",
      "iteration =  576, loss = 0.00716\n",
      "iteration =  577, loss = 0.00702\n",
      "iteration =  578, loss = 0.00688\n",
      "iteration =  579, loss = 0.00674\n",
      "iteration =  580, loss = 0.00661\n",
      "iteration =  581, loss = 0.00648\n",
      "iteration =  582, loss = 0.00635\n",
      "iteration =  583, loss = 0.00622\n",
      "iteration =  584, loss = 0.00609\n",
      "iteration =  585, loss = 0.00597\n",
      "iteration =  586, loss = 0.00585\n",
      "iteration =  587, loss = 0.00574\n",
      "iteration =  588, loss = 0.00562\n",
      "iteration =  589, loss = 0.00551\n",
      "iteration =  590, loss = 0.00540\n",
      "iteration =  591, loss = 0.00529\n",
      "iteration =  592, loss = 0.00518\n",
      "iteration =  593, loss = 0.00508\n",
      "iteration =  594, loss = 0.00498\n",
      "iteration =  595, loss = 0.00488\n",
      "iteration =  596, loss = 0.00478\n",
      "iteration =  597, loss = 0.00469\n",
      "iteration =  598, loss = 0.00459\n",
      "iteration =  599, loss = 0.00450\n",
      "iteration =  600, loss = 0.00441\n",
      "iteration =  601, loss = 0.00432\n",
      "iteration =  602, loss = 0.00424\n",
      "iteration =  603, loss = 0.00415\n",
      "iteration =  604, loss = 0.00407\n",
      "iteration =  605, loss = 0.00399\n",
      "iteration =  606, loss = 0.00391\n",
      "iteration =  607, loss = 0.00383\n",
      "iteration =  608, loss = 0.00375\n",
      "iteration =  609, loss = 0.00368\n",
      "iteration =  610, loss = 0.00360\n",
      "iteration =  611, loss = 0.00353\n",
      "iteration =  612, loss = 0.00346\n",
      "iteration =  613, loss = 0.00339\n",
      "iteration =  614, loss = 0.00332\n",
      "iteration =  615, loss = 0.00326\n",
      "iteration =  616, loss = 0.00319\n",
      "iteration =  617, loss = 0.00313\n",
      "iteration =  618, loss = 0.00307\n",
      "iteration =  619, loss = 0.00300\n",
      "iteration =  620, loss = 0.00294\n",
      "iteration =  621, loss = 0.00289\n",
      "iteration =  622, loss = 0.00283\n",
      "iteration =  623, loss = 0.00277\n",
      "iteration =  624, loss = 0.00272\n",
      "iteration =  625, loss = 0.00266\n",
      "iteration =  626, loss = 0.00261\n",
      "iteration =  627, loss = 0.00256\n",
      "iteration =  628, loss = 0.00251\n",
      "iteration =  629, loss = 0.00246\n",
      "iteration =  630, loss = 0.00241\n",
      "iteration =  631, loss = 0.00236\n",
      "iteration =  632, loss = 0.00231\n",
      "iteration =  633, loss = 0.00226\n",
      "iteration =  634, loss = 0.00222\n",
      "iteration =  635, loss = 0.00218\n",
      "iteration =  636, loss = 0.00213\n",
      "iteration =  637, loss = 0.00209\n",
      "iteration =  638, loss = 0.00205\n",
      "iteration =  639, loss = 0.00201\n",
      "iteration =  640, loss = 0.00197\n",
      "iteration =  641, loss = 0.00193\n",
      "iteration =  642, loss = 0.00189\n",
      "iteration =  643, loss = 0.00185\n",
      "iteration =  644, loss = 0.00181\n",
      "iteration =  645, loss = 0.00178\n",
      "iteration =  646, loss = 0.00174\n",
      "iteration =  647, loss = 0.00171\n",
      "iteration =  648, loss = 0.00167\n",
      "iteration =  649, loss = 0.00164\n",
      "iteration =  650, loss = 0.00161\n",
      "iteration =  651, loss = 0.00157\n",
      "iteration =  652, loss = 0.00154\n",
      "iteration =  653, loss = 0.00151\n",
      "iteration =  654, loss = 0.00148\n",
      "iteration =  655, loss = 0.00145\n",
      "iteration =  656, loss = 0.00142\n",
      "iteration =  657, loss = 0.00139\n",
      "iteration =  658, loss = 0.00137\n",
      "iteration =  659, loss = 0.00134\n",
      "iteration =  660, loss = 0.00131\n",
      "iteration =  661, loss = 0.00129\n",
      "iteration =  662, loss = 0.00126\n",
      "iteration =  663, loss = 0.00124\n",
      "iteration =  664, loss = 0.00121\n",
      "iteration =  665, loss = 0.00119\n",
      "iteration =  666, loss = 0.00116\n",
      "iteration =  667, loss = 0.00114\n",
      "iteration =  668, loss = 0.00112\n",
      "iteration =  669, loss = 0.00109\n",
      "iteration =  670, loss = 0.00107\n",
      "iteration =  671, loss = 0.00105\n",
      "iteration =  672, loss = 0.00103\n",
      "iteration =  673, loss = 0.00101\n",
      "iteration =  674, loss = 0.00099\n",
      "iteration =  675, loss = 0.00097\n",
      "iteration =  676, loss = 0.00095\n",
      "iteration =  677, loss = 0.00093\n",
      "iteration =  678, loss = 0.00091\n",
      "iteration =  679, loss = 0.00089\n",
      "iteration =  680, loss = 0.00088\n",
      "iteration =  681, loss = 0.00086\n",
      "iteration =  682, loss = 0.00084\n",
      "iteration =  683, loss = 0.00082\n",
      "iteration =  684, loss = 0.00081\n",
      "iteration =  685, loss = 0.00079\n",
      "iteration =  686, loss = 0.00078\n",
      "iteration =  687, loss = 0.00076\n",
      "iteration =  688, loss = 0.00075\n",
      "iteration =  689, loss = 0.00073\n",
      "iteration =  690, loss = 0.00072\n",
      "iteration =  691, loss = 0.00070\n",
      "iteration =  692, loss = 0.00069\n",
      "iteration =  693, loss = 0.00067\n",
      "iteration =  694, loss = 0.00066\n",
      "iteration =  695, loss = 0.00065\n",
      "iteration =  696, loss = 0.00063\n",
      "iteration =  697, loss = 0.00062\n",
      "iteration =  698, loss = 0.00061\n",
      "iteration =  699, loss = 0.00060\n",
      "iteration =  700, loss = 0.00058\n",
      "iteration =  701, loss = 0.00057\n",
      "iteration =  702, loss = 0.00056\n",
      "iteration =  703, loss = 0.00055\n",
      "iteration =  704, loss = 0.00054\n",
      "iteration =  705, loss = 0.00053\n",
      "iteration =  706, loss = 0.00052\n",
      "iteration =  707, loss = 0.00051\n",
      "iteration =  708, loss = 0.00050\n",
      "iteration =  709, loss = 0.00049\n",
      "iteration =  710, loss = 0.00048\n",
      "iteration =  711, loss = 0.00047\n",
      "iteration =  712, loss = 0.00046\n",
      "iteration =  713, loss = 0.00045\n",
      "iteration =  714, loss = 0.00044\n",
      "iteration =  715, loss = 0.00043\n",
      "iteration =  716, loss = 0.00042\n",
      "iteration =  717, loss = 0.00041\n",
      "iteration =  718, loss = 0.00041\n",
      "iteration =  719, loss = 0.00040\n",
      "iteration =  720, loss = 0.00039\n",
      "iteration =  721, loss = 0.00038\n",
      "iteration =  722, loss = 0.00038\n",
      "iteration =  723, loss = 0.00037\n",
      "iteration =  724, loss = 0.00036\n",
      "iteration =  725, loss = 0.00035\n",
      "iteration =  726, loss = 0.00035\n",
      "iteration =  727, loss = 0.00034\n",
      "iteration =  728, loss = 0.00033\n",
      "iteration =  729, loss = 0.00033\n",
      "iteration =  730, loss = 0.00032\n",
      "iteration =  731, loss = 0.00031\n",
      "iteration =  732, loss = 0.00031\n",
      "iteration =  733, loss = 0.00030\n",
      "iteration =  734, loss = 0.00029\n",
      "iteration =  735, loss = 0.00029\n",
      "iteration =  736, loss = 0.00028\n",
      "iteration =  737, loss = 0.00028\n",
      "iteration =  738, loss = 0.00027\n",
      "iteration =  739, loss = 0.00027\n",
      "iteration =  740, loss = 0.00026\n",
      "iteration =  741, loss = 0.00026\n",
      "iteration =  742, loss = 0.00025\n",
      "iteration =  743, loss = 0.00025\n",
      "iteration =  744, loss = 0.00024\n",
      "iteration =  745, loss = 0.00024\n",
      "iteration =  746, loss = 0.00023\n",
      "iteration =  747, loss = 0.00023\n",
      "iteration =  748, loss = 0.00022\n",
      "iteration =  749, loss = 0.00022\n",
      "iteration =  750, loss = 0.00021\n",
      "iteration =  751, loss = 0.00021\n",
      "iteration =  752, loss = 0.00020\n",
      "iteration =  753, loss = 0.00020\n",
      "iteration =  754, loss = 0.00020\n",
      "iteration =  755, loss = 0.00019\n",
      "iteration =  756, loss = 0.00019\n",
      "iteration =  757, loss = 0.00018\n",
      "iteration =  758, loss = 0.00018\n",
      "iteration =  759, loss = 0.00018\n",
      "iteration =  760, loss = 0.00017\n",
      "iteration =  761, loss = 0.00017\n",
      "iteration =  762, loss = 0.00017\n",
      "iteration =  763, loss = 0.00016\n",
      "iteration =  764, loss = 0.00016\n",
      "iteration =  765, loss = 0.00016\n",
      "iteration =  766, loss = 0.00015\n",
      "iteration =  767, loss = 0.00015\n",
      "iteration =  768, loss = 0.00015\n",
      "iteration =  769, loss = 0.00015\n",
      "iteration =  770, loss = 0.00014\n",
      "iteration =  771, loss = 0.00014\n",
      "iteration =  772, loss = 0.00014\n",
      "iteration =  773, loss = 0.00013\n",
      "iteration =  774, loss = 0.00013\n",
      "iteration =  775, loss = 0.00013\n",
      "iteration =  776, loss = 0.00013\n",
      "iteration =  777, loss = 0.00012\n",
      "iteration =  778, loss = 0.00012\n",
      "iteration =  779, loss = 0.00012\n",
      "iteration =  780, loss = 0.00012\n",
      "iteration =  781, loss = 0.00011\n",
      "iteration =  782, loss = 0.00011\n",
      "iteration =  783, loss = 0.00011\n",
      "iteration =  784, loss = 0.00011\n",
      "iteration =  785, loss = 0.00011\n",
      "iteration =  786, loss = 0.00010\n",
      "iteration =  787, loss = 0.00010\n",
      "iteration =  788, loss = 0.00010\n",
      "iteration =  789, loss = 0.00010\n",
      "iteration =  790, loss = 0.00009\n",
      "iteration =  791, loss = 0.00009\n",
      "iteration =  792, loss = 0.00009\n",
      "iteration =  793, loss = 0.00009\n",
      "iteration =  794, loss = 0.00009\n",
      "iteration =  795, loss = 0.00009\n",
      "iteration =  796, loss = 0.00008\n",
      "iteration =  797, loss = 0.00008\n",
      "iteration =  798, loss = 0.00008\n",
      "iteration =  799, loss = 0.00008\n",
      "iteration =  800, loss = 0.00008\n",
      "iteration =  801, loss = 0.00008\n",
      "iteration =  802, loss = 0.00007\n",
      "iteration =  803, loss = 0.00007\n",
      "iteration =  804, loss = 0.00007\n",
      "iteration =  805, loss = 0.00007\n",
      "iteration =  806, loss = 0.00007\n",
      "iteration =  807, loss = 0.00007\n",
      "iteration =  808, loss = 0.00007\n",
      "iteration =  809, loss = 0.00006\n",
      "iteration =  810, loss = 0.00006\n",
      "iteration =  811, loss = 0.00006\n",
      "iteration =  812, loss = 0.00006\n",
      "iteration =  813, loss = 0.00006\n",
      "iteration =  814, loss = 0.00006\n",
      "iteration =  815, loss = 0.00006\n",
      "iteration =  816, loss = 0.00006\n",
      "iteration =  817, loss = 0.00006\n",
      "iteration =  818, loss = 0.00005\n",
      "iteration =  819, loss = 0.00005\n",
      "iteration =  820, loss = 0.00005\n",
      "iteration =  821, loss = 0.00005\n",
      "iteration =  822, loss = 0.00005\n",
      "iteration =  823, loss = 0.00005\n",
      "iteration =  824, loss = 0.00005\n",
      "iteration =  825, loss = 0.00005\n",
      "iteration =  826, loss = 0.00005\n",
      "iteration =  827, loss = 0.00004\n",
      "iteration =  828, loss = 0.00004\n",
      "iteration =  829, loss = 0.00004\n",
      "iteration =  830, loss = 0.00004\n",
      "iteration =  831, loss = 0.00004\n",
      "iteration =  832, loss = 0.00004\n",
      "iteration =  833, loss = 0.00004\n",
      "iteration =  834, loss = 0.00004\n",
      "iteration =  835, loss = 0.00004\n",
      "iteration =  836, loss = 0.00004\n",
      "iteration =  837, loss = 0.00004\n",
      "iteration =  838, loss = 0.00004\n",
      "iteration =  839, loss = 0.00004\n",
      "iteration =  840, loss = 0.00003\n",
      "iteration =  841, loss = 0.00003\n",
      "iteration =  842, loss = 0.00003\n",
      "iteration =  843, loss = 0.00003\n",
      "iteration =  844, loss = 0.00003\n",
      "iteration =  845, loss = 0.00003\n",
      "iteration =  846, loss = 0.00003\n",
      "iteration =  847, loss = 0.00003\n",
      "iteration =  848, loss = 0.00003\n",
      "iteration =  849, loss = 0.00003\n",
      "iteration =  850, loss = 0.00003\n",
      "iteration =  851, loss = 0.00003\n",
      "iteration =  852, loss = 0.00003\n",
      "iteration =  853, loss = 0.00003\n",
      "iteration =  854, loss = 0.00003\n",
      "iteration =  855, loss = 0.00003\n",
      "iteration =  856, loss = 0.00003\n",
      "iteration =  857, loss = 0.00002\n",
      "iteration =  858, loss = 0.00002\n",
      "iteration =  859, loss = 0.00002\n",
      "iteration =  860, loss = 0.00002\n",
      "iteration =  861, loss = 0.00002\n",
      "iteration =  862, loss = 0.00002\n",
      "iteration =  863, loss = 0.00002\n",
      "iteration =  864, loss = 0.00002\n",
      "iteration =  865, loss = 0.00002\n",
      "iteration =  866, loss = 0.00002\n",
      "iteration =  867, loss = 0.00002\n",
      "iteration =  868, loss = 0.00002\n",
      "iteration =  869, loss = 0.00002\n",
      "iteration =  870, loss = 0.00002\n",
      "iteration =  871, loss = 0.00002\n",
      "iteration =  872, loss = 0.00002\n",
      "iteration =  873, loss = 0.00002\n",
      "iteration =  874, loss = 0.00002\n",
      "iteration =  875, loss = 0.00002\n",
      "iteration =  876, loss = 0.00002\n",
      "iteration =  877, loss = 0.00002\n",
      "iteration =  878, loss = 0.00002\n",
      "iteration =  879, loss = 0.00002\n",
      "iteration =  880, loss = 0.00002\n",
      "iteration =  881, loss = 0.00002\n",
      "iteration =  882, loss = 0.00001\n",
      "iteration =  883, loss = 0.00001\n",
      "iteration =  884, loss = 0.00001\n",
      "iteration =  885, loss = 0.00001\n",
      "iteration =  886, loss = 0.00001\n",
      "iteration =  887, loss = 0.00001\n",
      "iteration =  888, loss = 0.00001\n",
      "iteration =  889, loss = 0.00001\n",
      "iteration =  890, loss = 0.00001\n",
      "iteration =  891, loss = 0.00001\n",
      "iteration =  892, loss = 0.00001\n",
      "iteration =  893, loss = 0.00001\n",
      "iteration =  894, loss = 0.00001\n",
      "iteration =  895, loss = 0.00001\n",
      "iteration =  896, loss = 0.00001\n",
      "iteration =  897, loss = 0.00001\n",
      "iteration =  898, loss = 0.00001\n",
      "iteration =  899, loss = 0.00001\n",
      "iteration =  900, loss = 0.00001\n",
      "iteration =  901, loss = 0.00001\n",
      "iteration =  902, loss = 0.00001\n",
      "iteration =  903, loss = 0.00001\n",
      "iteration =  904, loss = 0.00001\n",
      "iteration =  905, loss = 0.00001\n",
      "iteration =  906, loss = 0.00001\n",
      "iteration =  907, loss = 0.00001\n",
      "iteration =  908, loss = 0.00001\n",
      "iteration =  909, loss = 0.00001\n",
      "iteration =  910, loss = 0.00001\n",
      "iteration =  911, loss = 0.00001\n",
      "iteration =  912, loss = 0.00001\n",
      "iteration =  913, loss = 0.00001\n",
      "iteration =  914, loss = 0.00001\n",
      "iteration =  915, loss = 0.00001\n",
      "iteration =  916, loss = 0.00001\n",
      "iteration =  917, loss = 0.00001\n",
      "iteration =  918, loss = 0.00001\n",
      "iteration =  919, loss = 0.00001\n",
      "iteration =  920, loss = 0.00001\n",
      "iteration =  921, loss = 0.00001\n",
      "iteration =  922, loss = 0.00001\n",
      "iteration =  923, loss = 0.00001\n",
      "iteration =  924, loss = 0.00001\n",
      "iteration =  925, loss = 0.00001\n",
      "iteration =  926, loss = 0.00001\n",
      "iteration =  927, loss = 0.00001\n",
      "iteration =  928, loss = 0.00001\n",
      "iteration =  929, loss = 0.00001\n",
      "iteration =  930, loss = 0.00001\n",
      "iteration =  931, loss = 0.00001\n",
      "iteration =  932, loss = 0.00001\n",
      "iteration =  933, loss = 0.00001\n",
      "iteration =  934, loss = 0.00001\n",
      "iteration =  935, loss = 0.00001\n",
      "iteration =  936, loss = 0.00000\n",
      "iteration =  937, loss = 0.00000\n",
      "iteration =  938, loss = 0.00000\n",
      "iteration =  939, loss = 0.00000\n",
      "iteration =  940, loss = 0.00000\n",
      "iteration =  941, loss = 0.00000\n",
      "iteration =  942, loss = 0.00000\n",
      "iteration =  943, loss = 0.00000\n",
      "iteration =  944, loss = 0.00000\n",
      "iteration =  945, loss = 0.00000\n",
      "iteration =  946, loss = 0.00000\n",
      "iteration =  947, loss = 0.00000\n",
      "iteration =  948, loss = 0.00000\n",
      "iteration =  949, loss = 0.00000\n",
      "iteration =  950, loss = 0.00000\n",
      "iteration =  951, loss = 0.00000\n",
      "iteration =  952, loss = 0.00000\n",
      "iteration =  953, loss = 0.00000\n",
      "iteration =  954, loss = 0.00000\n",
      "iteration =  955, loss = 0.00000\n",
      "iteration =  956, loss = 0.00000\n",
      "iteration =  957, loss = 0.00000\n",
      "iteration =  958, loss = 0.00000\n",
      "iteration =  959, loss = 0.00000\n",
      "iteration =  960, loss = 0.00000\n",
      "iteration =  961, loss = 0.00000\n",
      "iteration =  962, loss = 0.00000\n",
      "iteration =  963, loss = 0.00000\n",
      "iteration =  964, loss = 0.00000\n",
      "iteration =  965, loss = 0.00000\n",
      "iteration =  966, loss = 0.00000\n",
      "iteration =  967, loss = 0.00000\n",
      "iteration =  968, loss = 0.00000\n",
      "iteration =  969, loss = 0.00000\n",
      "iteration =  970, loss = 0.00000\n",
      "iteration =  971, loss = 0.00000\n",
      "iteration =  972, loss = 0.00000\n",
      "iteration =  973, loss = 0.00000\n",
      "iteration =  974, loss = 0.00000\n",
      "iteration =  975, loss = 0.00000\n",
      "iteration =  976, loss = 0.00000\n",
      "iteration =  977, loss = 0.00000\n",
      "iteration =  978, loss = 0.00000\n",
      "iteration =  979, loss = 0.00000\n",
      "iteration =  980, loss = 0.00000\n",
      "iteration =  981, loss = 0.00000\n",
      "iteration =  982, loss = 0.00000\n",
      "iteration =  983, loss = 0.00000\n",
      "iteration =  984, loss = 0.00000\n",
      "iteration =  985, loss = 0.00000\n",
      "iteration =  986, loss = 0.00000\n",
      "iteration =  987, loss = 0.00000\n",
      "iteration =  988, loss = 0.00000\n",
      "iteration =  989, loss = 0.00000\n",
      "iteration =  990, loss = 0.00000\n",
      "iteration =  991, loss = 0.00000\n",
      "iteration =  992, loss = 0.00000\n",
      "iteration =  993, loss = 0.00000\n",
      "iteration =  994, loss = 0.00000\n",
      "iteration =  995, loss = 0.00000\n",
      "iteration =  996, loss = 0.00000\n",
      "iteration =  997, loss = 0.00000\n",
      "iteration =  998, loss = 0.00000\n",
      "iteration =  999, loss = 0.00000\n"
     ]
    }
   ],
   "source": [
    "num_iteration       = 1000\n",
    "learning_rate       = 0.01\n",
    "\n",
    "theta0              = 0\n",
    "theta1              = 0\n",
    "\n",
    "theta0_iteration    = np.zeros(num_iteration)\n",
    "theta1_iteration    = np.zeros(num_iteration)\n",
    "loss_iteration      = np.zeros(num_iteration)\n",
    "\n",
    "for i in range(num_iteration):\n",
    "\n",
    "    theta0  = theta0 - learning_rate * compute_gradient_theta0(x, y, theta0, theta1)\n",
    "    theta1  = theta1 - learning_rate * compute_gradient_theta1(x, y, theta0, theta1)\n",
    "    loss    = compute_loss(x, y, theta0, theta1)\n",
    "\n",
    "    theta0_iteration[i] = theta0\n",
    "    theta1_iteration[i] = theta1\n",
    "    loss_iteration[i]   = loss\n",
    "\n",
    "    print(\"iteration = %4d, loss = %5.5f\" % (i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = theta0_iteration[-1] + theta1_iteration[-1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_regression(x, y, f):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('linear regression result')\n",
    "    plt.plot(x, f, '-', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('loss curve')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_parameter(theta0_iteration, theta1_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('model parameter')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-120-aafdae4f6d48>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-120-aafdae4f6d48>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    grid_theta0, grid_theta1 =\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X0  = np.arange(-10, 10, 0.1)\n",
    "X1  = np.arange(-10, 10, 0.1)\n",
    "\n",
    "grid_theta0, grid_theta1 = \n",
    "\n",
    "grid_loss   = \n",
    "\n",
    "\n",
    "def plot_loss_surface(grid_theta0, grid_theta1, grid_loss):\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.title('loss surface')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 01. plot the input data in blue point and the regression result in red curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_regression(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 02. plot the values of the model parameters $\\theta_0$ in blue curve and $\\theta_1$ in green curve over the gradient descent iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta0_iteration, theta1_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 03. plot the loss values $\\mathcal{L}(\\theta)$ in red curve over the gradient descent iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 04. plot the loss surface in 3-dimension surface where $x$-axis represents $\\theta_0$, $y$-axis represents $\\theta_1$ and $z$-axis represents $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_surface(grid_theta0, grid_theta1, grid_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
